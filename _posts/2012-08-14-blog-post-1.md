---
title: '高精度な日本語マルチモーダル大規模言語モデルの構築にむけたデータセットの検討 (NLP2025)'
permalink: /posts/Japanese_MLLM
---

田中 幹大, 朱 佩菲, 横尾 修平 (LINEヤフー株式会社) [[paper]](https://mikittt.github.io/files/jmllm.pdf)

このページでは、NLPの論文内容に加え、新しく日本語MLLMのベンチマークとして提案するJIC-VQAベンチマークについても紹介する。

- [概要](#概要)
- [実験結果](#実験結果)
- [JIC-VQAベンチマークの作成](#JIC-VQAベンチマークの作成)


<a id='概要'>概要</a>
------
近年、大規模言語モデル　(LLM) に視覚情報を統合した、マルチモーダル大規模言語モデル（MLLM）が注目を集めており、その応用範囲は急速に拡大している。しかし、日本ドメインに特化したMLLM を作る上で、英語のデータに比べて公開データが少ない課題がある。本研究では、高精度な日本語MLLMを構築するためのデータセットの作成方法について検討し、実験を行った。構築したモデルは、日本ドメインの画像理解を問うベンチマークにおいて、他のモデルよりも優位な結果を示し、その有効性を実証した。

<a id='実験結果'>実験結果</a>
------
提案モデルは、日本ドメインの画像を扱うHeron-BenchとJA-VLM-Benchにおいて既存手法を上回る性能を達成した。
<img src='/images/japanese_mllm/quantitative.png'>

本研究では[CALM3-22B](https://huggingface.co/cyberagent/calm3-22b-chat)を用いてMLLMを構築した。本研究で構築したデータセットの有用性を確認するために、VILA-jpと同様[llm-jp-3-13b-instruct](https://huggingface.co/llm-jp/llm-jp-3-13b-instruct)を用いた実験を行った。

llm-jp-3-13b-instructを用いた時は、提案データセットに対して適切なフィルタリングを行うことでVILA-jpを上回る性能を得た(表上から3番目)。ここで、フィルタリングは作成したデータの中で、解答文に疑問文を含むものを除く処理を行った。また、llm-jp-3-13b-instructよりも大きなCALM3-22Bがこれらのベンチマークでも有効であることが分かった。CALM3-22Bを用いた時はフィルタリングは効果を示さず、これは大きいLLMはノイズに頑健な性質を持つためと考える。

| Models       | Heron-Bench <br> LLM Average (%) | JA-VLM-Bench <br> LLM (/5.0) |
|-------------|----------------------------------|------------------------------|
| VILA-jp (SigLIP + llm-jp-3-13b-instruct)         | 57.2       | 3.69|
| (提案データ)  SigLIP + llm-jp-3-13b-instruct         | 56.6       | 3.62|
| (提案データ+filtering) SigLIP + llm-jp-3-13b-instruct | 60.4       | 3.7|
| (提案データ)     SigLIP + CALM3-22B        | 63.3       | 3.82|
| (提案データ+filtering)     SigLIP + CALM3-22B        | 62.4       | 3.7|

定性的結果例を以下に示す。
<img src='/images/japanese_mllm/qualitative.png'>


<a id='JIC-VQAベンチマークの作成'>JIC-VQAベンチマークの作成</a>
------

日本語MLLMを作る上で、日本ドメインの画像を認識できるかは最も基礎的で重要な能力の一つである。これまで日本ドメイン画像を用いて、Heron-BenchやJA-VLM-Benchに加えて、JMMMUといったベンチマークが提案されてきた。しかしこれらのベンチマークでは、知識を問うものなどの多様な質問も含まれているため、日本ドメインの画像認識に焦点を置いた評価ができない課題があった。

そこで、Recruit社が日本語CLIPの評価のために公開した[japanese-image-classification-evaluation-dataset](https://huggingface.co/datasets/recruit-jp/japanese-image-classification-evaluation-dataset)を用いて4択式の7,654件の質問を付与した、Japanese-Image-Classification-VQA(JIC-VQA)と呼ぶベンチマークによる評価を提案する。4つの選択肢のうち誤りの回答は、正解と最も類似しているクラスをクラス名の候補からLLMによって選択して用意した。

提案するJIC-VQAベンチマークは、元のデータセットと同じで、jaflower30(日本の花30種)・jafood101(日本の食材、料理101種)・
jalandmark10(日本のランドマーク10種)・jafacility20(日本の施設20種)から構成される。

ベンチマークの例を以下に示す。
<img src='/images/japanese_mllm/JIC-QA.png'>

日本語MLLMの評価に一般的に用いられるベンチマークとの比較を以下の表に示す。提案するJIC-VQAは日本ドメインの画像の常識的なレベルの理解を問う、大きなベンチマークとなっている。

| ベンチマーク         | 画像ドメイン | 質問数 | レベル | 評価方法 | 
|-------------|---------------|----------------|----------------|----------------|
|[Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)| 日本 | 102 | 常識 | LLM |
|[JA-VLM-Bench](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild) | 日本 | 60| 常識 | 自動評価 or LLM |
|[JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)| 海外 | 4,000 | 常識 | 自動評価 or LLM |
|[JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU) | 日本 | 1,320| 専門的 | 正解率 |
| JIC-VQA (提案)| 日本 | 7,654| 常識 | 正解率 |



評価結果を以下の表に示す。clip-japanese-base(ViT-L/14@336px)とは、日本ドメインに特化したCLIPである[clip-japanese-base](https://huggingface.co/line-corporation/clip-japanese-base)のモデルサイズや解像度を大きくしたバージョンである。MLLMの画像エンコーダーとしてclip-japanese-base(ViT-L/14@336px)を用いたモデルは、海外ドメインで主に訓練されている[SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384)を用いたモデルの性能を大きく上回り、特に食べ物やランドマークの認識で差がついた。

その他の画像エンコーダーを用いたモデルの結果として、[Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)と[InternViT-300M-448px](https://huggingface.co/OpenGVLab/InternViT-300M-448px)を用いたInternVL2.5-8Bの結果を示す。Qwen2.5-VL-7B-Instructは独自の画像エンコーダーを使用しており、学習データなどの詳細は公開されていないが、日本のランドマークの認識能力も高いことから日本ドメインの画像も多く学習されていることが推測される。しかしQwen2.5-VL-7B-Instructでも食べ物の認識の正解率は低く、日本ドメインの画像認識能力に課題を残すことが分かる。

| モデル     | jaflower30|jafood101|jalandmark10|jafacility20|平均|
|-------------|---------------|----------------|------------------|----------------|--------------|
|[LLaVA-CALM2-SigLIP](https://huggingface.co/cyberagent/llava-calm2-siglip) | 0.61 | 0.55 | 0.42 | 0.77 | 0.59 |
|[VILA-jp](https://huggingface.co/llm-jp/llm-jp-3-vila-14b) (SigLIP + llm-jp-3-13b-instruct) | 0.91 | 0.76 | 0.74 | 0.89 | 0.83 |
|[Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) |0.91|0.79|<b>0.92</b>|<b>0.93</b>|0.87|
|[InternVL2.5-8B](https://huggingface.co/OpenGVLab/InternVL2_5-8B)|0.48|0.63|0.71|0.81|0.66|
|(提案データ) SigLIP + CALM3-22B               | 0.92|0.84|0.79|0.86|0.85|
|(提案データ) clip-japanese-base(ViT-L/14@336px) + CALM3-22B | <b>0.95</b>|<b>0.91</b>|0.89|<b>0.93</b>|<b>0.92</b>|

定性的な結果を以下の図に示す。これらの結果からも、海外ドメインを中心として学習されたSigLIPを画像エンコーダーを用いると、豊富な日本ドメインの知識を持つLLMと合わせたMLLMを学習しても日本ドメインの画像理解に課題を残すことがわかり、日本ドメインにおいて強力な画像エンコーダーの構築が重要であることが分かる。

<img src='/images/japanese_mllm/new-res3.png' width="85%">